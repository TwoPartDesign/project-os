---
description: "Compare and review competing implementations side-by-side"
---

# Competitive Review

Review and compare competing implementations generated by `/workflows:compete`.

## Input
Read the competition results: `docs/specs/$ARGUMENTS/tasks/TN/compete-*.md`
The user specifies: `/workflows:compete-review <feature> <task_id>`

## Step 1: Load all approaches

For each `compete-<strategy>.md` file:
1. Read the implementation summary (this is the primary source of truth — it persists after worktree cleanup)
2. If the corresponding worktree is still available, read the actual diff for detailed code review. If not, rely on the summary and any saved diffs in the completion report.
3. Note test results

## Step 2: Deep comparison

Spawn a reviewer sub-agent for each approach (parallel, isolated):

"You are reviewing a competitive implementation.

Strategy: [STRATEGY NAME]
Task spec: [TASK DESCRIPTION]
Implementation:
[DIFF OR FILE CONTENTS]

Evaluate on these axes:
1. **Correctness**: Does it satisfy all acceptance criteria?
2. **Simplicity**: Is it the simplest solution that works?
3. **Robustness**: How does it handle edge cases and errors?
4. **Readability**: Can another developer understand it quickly?
5. **Testability**: Are tests thorough and maintainable?
6. **Convention alignment**: Does it follow CLAUDE.md patterns?

Score each axis 1-5. Provide specific code references for your scoring."

## Step 3: Synthesize

Create a unified comparison matrix:

```
                  Literal   Minimal   Extensible
Correctness       [1-5]     [1-5]     [1-5]
Simplicity        [1-5]     [1-5]     [1-5]
Robustness        [1-5]     [1-5]     [1-5]
Readability       [1-5]     [1-5]     [1-5]
Testability       [1-5]     [1-5]     [1-5]
Convention fit    [1-5]     [1-5]     [1-5]
──────────────────────────────────────────────
TOTAL             [sum]     [sum]     [sum]
```

## Step 4: Recommendation

Based on the scores and the project's principle "Ship working software over perfect software":
- Recommend the approach with the best balance
- Call out if any approach is clearly superior
- Flag if all approaches have the same weakness (task spec issue)

Present to the user for final selection.

## Output
Update `docs/specs/$ARGUMENTS/tasks/TN/compete-comparison.md` with the detailed review scores.
